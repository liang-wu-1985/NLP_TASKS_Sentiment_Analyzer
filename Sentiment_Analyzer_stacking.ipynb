{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import neologdn\n",
    "import os\n",
    "import urllib.request\n",
    "from matplotlib.font_manager import FontProperties\n",
    "import MeCab\n",
    "from __future__ import unicode_literals\n",
    "import re\n",
    "import unicodedata\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import os\n",
    "import codecs\n",
    "plt.style.use('fivethirtyeight')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "#chaneg dummies to matrix\n",
    "from scipy.sparse import csr_matrix,coo_matrix\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import pickle\n",
    "\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import  roc_auc_score,f1_score\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook as tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.layers import Input, Embedding, Dense, Conv2D, MaxPool2D, concatenate,Conv1D, MaxPool1D\n",
    "from keras.layers import Reshape, Flatten, Concatenate, Dropout, SpatialDropout1D\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint, ReduceLROnPlateau,Callback\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "import os\n",
    "import tarfile\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from tqdm._tqdm_notebook import tqdm_notebook\n",
    "tqdm_notebook.pandas(desc=\"hoge progress: \")\n",
    "\n",
    "import time\n",
    "import gensim\n",
    "tqdm.pandas()\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "from sudachipy import tokenizer\n",
    "from sudachipy import dictionary\n",
    "\n",
    "\n",
    "tokenizer_obj = dictionary.Dictionary().create()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_func(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    \n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class f1_callback(Callback):\n",
    "    def __init__(self,training_data,validation_data):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x, verbose=0)\n",
    "        roc = roc_auc_score(self.y, y_pred)\n",
    "        f1 = f1_func(self.y, y_pred)        \n",
    "        logs['roc_auc'] = roc_auc_score(self.y, y_pred)\n",
    "        logs['f1']= f1_func (self.y, y_pred)\n",
    "\n",
    "        y_pred_val = self.model.predict(self.x_val, verbose=0)\n",
    "        roc_val = roc_auc_score(self.y_val, y_pred_val)\n",
    "        f1_val = f1_func(self.y_val, y_pred_val)  \n",
    "        logs['roc_auc_val'] = roc_auc_score(self.y_val, y_pred_val)\n",
    "        logs['f1']= f1_func(selfy_valy, y_pred_val)\n",
    "\n",
    "        print('\\roc_auc: %s - roc_auc_val: %s - f1: %s - f1_val: %s' % (str(round(roc,5)),str(round(roc_val,5)),str(round(f1,5)),str(round(f1_val,5))), end=10*' '+'\\n')\n",
    "        return\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlinking sudachidict\r\n",
      "sudachidict unlinked\r\n",
      "default dict package = sudachidict_full\r\n"
     ]
    }
   ],
   "source": [
    "!sudachipy link -t full\n",
    "mode = tokenizer.Tokenizer.SplitMode.A\n",
    "def wakati_by_sudachi(txt):\n",
    "    processed=[m.normalized_form() for m in tokenizer_obj.tokenize(txt, mode)]\n",
    "    return \" \".join(processed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_chars(row):\n",
    "    cust_list=['\\d+','■','\\n','#','◇','①','②','③','④','【】','】','【','☆','_','%','「','」','★','/']\n",
    "    del_list = string.ascii_letters+'\"#$%&\\'()*+,-./:;<=>@[\\\\]^_`{|}~'\n",
    "    for i in del_list:\n",
    "        row = row.str.replace(i,'')\n",
    "    for i in cust_list :\n",
    "        row = row.str.replace(i,'')\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_contractions(text):\n",
    "    mapping ={\"矢張り\":\"やはり\",\"迚も\":\"とても\",\"迚も\":\"とても\",\"此れ\":\"これ\",\"其れ\":\"それ\",\"此の\":\"この\",\"此の\":\"この\",\"可成\":\"かなり\",\"兎に角\":\"とにかく\",\"態々\":\"わざわざ\"\n",
    "             ,\"です\":\"\",\"だ\":\"\",\"ます\":\"\",\"て\":\"\",\"た\":\"\"}\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embeddings(path):\n",
    "    with open(path,'rb') as f:\n",
    "        emb_arr = pickle.load(f)\n",
    "    return emb_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator \n",
    "\n",
    "def check_coverage(vocab,embeddings_index):\n",
    "    a = {}\n",
    "    oov = {}\n",
    "    k = 0\n",
    "    i = 0\n",
    "    for word in tqdm(vocab):\n",
    "        try:\n",
    "            a[word] = embeddings_index[word]\n",
    "            k += vocab[word]\n",
    "        except:\n",
    "\n",
    "            oov[word] = vocab[word]\n",
    "            i += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(a) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(k / (k + i)))\n",
    "    sorted_x = sorted(oov.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return sorted_x\n",
    "\n",
    "def build_vocab(sentences, verbose =  True):\n",
    "    \"\"\"\n",
    "    :param sentences: list of list of words\n",
    "    :return: dictionary of words and their count\n",
    "    \"\"\"\n",
    "    vocab = {}\n",
    "    for sentence in tqdm(sentences, disable = (not verbose)):\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_comment_data():\n",
    "    return pd.read_csv('/home/ec2-user/SageMaker/Notebooks_For_CX_Usecases/Sentiment_Analyzer/raw_data/sum_data_for_sentiment.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel=load_comment_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel['comment'] = remove_chars(df_mutilabel['comment'])\n",
    "df_mutilabel['comment'].replace('', np.nan, inplace=True)\n",
    "df_mutilabel.dropna(inplace=True)\n",
    "df_mutilabel['comment']=df_mutilabel['comment'].apply(neologdn.normalize)\n",
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_mutilabel['comment'] = pool.map(wakati_by_sudachi,df_mutilabel['comment'])\n",
    "pool.close() \n",
    "pool.join()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = Pool(processes=multiprocessing.cpu_count())\n",
    "df_mutilabel['comment'] = pool.map(clean_contractions,df_mutilabel['comment'])\n",
    "pool.close() \n",
    "pool.join()\n",
    "df_mutilabel['comment'].replace('', np.nan, inplace=True)\n",
    "df_mutilabel.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel['length']=df_mutilabel['comment'].apply(lambda x:len(set(x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel.drop_duplicates(subset='comment', keep='last', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time\n",
    "fasttext_embeddings=load_embeddings('/home/ec2-user/SageMaker/word2vector/fasttext_300d.pkl')\n",
    "print('loaded '+ str(len(fasttext_embeddings)) +' word vectors in {time.time()-tic}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(list(df_mutilabel['comment'].apply(lambda x:x.split()))) #use split to present word\n",
    "oov = check_coverage(vocab,fasttext_embeddings)\n",
    "oov[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## some config values \n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 7231 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 150 # max number of words in a question to use\n",
    "\n",
    "def load_split_data(df):\n",
    "    \n",
    "    train_df, val_df = train_test_split(df, test_size=0.3, random_state=2018)\n",
    "\n",
    "    ## fill up the missing values\n",
    "    train_X = train_df[\"comment\"].fillna(\"_##_\").values\n",
    "    train_meta_X=train_df[['recommendation','satisfication','length']].astype(int).values\n",
    "    val_X = val_df[\"comment\"].fillna(\"_##_\").values\n",
    "    val_meta_X=val_df[['recommendation','satisfication','length']].astype(int).values\n",
    "    #test_X = test_df[\"comment\"].fillna(\"_##_\").values\n",
    "\n",
    "    ## Tokenize the sentences\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(list(train_X)+list(val_X))\n",
    "    train_X = tokenizer.texts_to_sequences(train_X)\n",
    "    val_X = tokenizer.texts_to_sequences(val_X)\n",
    "    #test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "    ## Pad the sentences \n",
    "    train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "    val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "    #test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    ## Get the target values\n",
    "    train_y = train_df['label'].astype(int).values\n",
    "    val_y = val_df['label'].astype(int).values\n",
    "    \n",
    "    #shuffling the data\n",
    "    np.random.seed(2018)\n",
    "    trn_idx = np.random.permutation(len(train_X))\n",
    "    val_idx = np.random.permutation(len(val_X))\n",
    "\n",
    "    train_X = train_X[trn_idx]\n",
    "    val_X = val_X[val_idx]\n",
    "    train_y = train_y[trn_idx]\n",
    "    val_y = val_y[val_idx]  \n",
    "    \n",
    "    with open('tokenizer/newstart_sudachi.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return train_X, train_meta_X,val_X,val_meta_X,train_y, val_y, tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext(word_index,wv):\n",
    "\n",
    "    #配布されていたfastTextの次元数は300だったので300\n",
    "    EMBEDDING_DIM = 300\n",
    "    #embedding_metrix = np.zeros(top_vocab+1,EMBEDDING_DIM)\n",
    "    nb_words = len(word_index)+1\n",
    "    embedding_matrix = np.random.uniform(size=(nb_words,EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_vector = wv[word]\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except KeyError:\n",
    "            #print(\"Word: [\",word,\"] not in wvmodel! Use random embedding instead.\")\n",
    "            continue \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    result = df.copy()\n",
    "    cols=['recommendation','satisfication','length']\n",
    "    for feature_name in cols:\n",
    "        max_value = df[feature_name].max()\n",
    "        min_value = df[feature_name].min()\n",
    "        result[feature_name] = (df[feature_name] - min_value) / (max_value - min_value)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mutilabel=normalize(df_mutilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_meta_X,val_X,val_meta_X,train_y, val_y, dic_word_index=load_split_data(df_mutilabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix=load_fasttext(dic_word_index,fasttext_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoints = ModelCheckpoint('models/newstart_bestf1_weights.hdf5', monitor=\"val_f1\", mode=\"max\", verbose=False, save_best_only=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_f1', factor=0.1, patience=2, verbose=0, min_lr=0.000001)\n",
    "callbacks = [\n",
    "    #f1_callback(training_data=(train_X, train_y),validation_data=(val_X, val_y)),\n",
    "    EarlyStopping(patience=2, monitor='val_f1',mode='max'),\n",
    "    checkpoints,\n",
    "    reduce_lr\n",
    "]\n",
    "#imblanced data points \n",
    "cw_dict={1:len(train_y)/list(train_y).count(1),0:len(train_y)/list(train_y).count(0)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn_statistics(embedding_matrix):\n",
    "    \n",
    "    embed_size=embedding_matrix.shape[1]\n",
    "    \n",
    "    nlp_input = Input(shape=(maxlen,), name='nlp_input')\n",
    "    meta_input = Input(shape=(3,), name='meta_input')\n",
    "    emb = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1], weights=[embedding_matrix])(nlp_input)\n",
    "    nlp_out = Bidirectional(CuDNNLSTM(128,kernel_regularizer=regularizers.l2(0.01)))(emb)\n",
    "    nlp_out = Dropout(0.2)(nlp_out)\n",
    "    x = concatenate([nlp_out, meta_input])\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = Model(inputs=[nlp_input,meta_input], outputs=[x])\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',f1_func])\n",
    "    \n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnn(embedding_matrix):\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 36\n",
    "    embed_size=embedding_matrix.shape[1]\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1], weights=[embedding_matrix])(inp)\n",
    "    x = Reshape((maxlen, embed_size, 1))(x)\n",
    "    #x = Bidirectional(CuDNNLSTM(256,return_sequences=True))(x)\n",
    "    \n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv2D(num_filters, kernel_size=(filter_sizes[i], embed_size),\n",
    "                                     kernel_initializer='he_normal', activation='elu')(x)\n",
    "        maxpool_pool.append(MaxPool2D(pool_size=(maxlen - filter_sizes[i] + 1, 1))(conv))\n",
    "\n",
    "\n",
    "    z = Concatenate(axis=1)(maxpool_pool)   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "\n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',f1_func])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bilstm_cnn(embedding_matrix):\n",
    "    filter_sizes = [1,2,3,5]\n",
    "    num_filters = 36\n",
    "    embed_size=embedding_matrix.shape[1]\n",
    "\n",
    "    inp = Input(shape=(maxlen,))\n",
    "    x = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1], weights=[embedding_matrix])(inp)\n",
    "    #x = Reshape((maxlen, embed_size, 1))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(256,return_sequences=True))(x)\n",
    "    \n",
    "    maxpool_pool = []\n",
    "    for i in range(len(filter_sizes)):\n",
    "        conv = Conv1D(num_filters, kernel_size=(filter_sizes[i]),\n",
    "                                     kernel_initializer='he_normal', activation='elu')(x)\n",
    "        maxpool_pool.append(MaxPool1D(pool_size=(maxlen - filter_sizes[i] + 1))(conv))\n",
    "\n",
    "    z = Concatenate(axis=1)(maxpool_pool)   \n",
    "    z = Flatten()(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "\n",
    "    outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.summary()\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy',f1_func])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_gru_du(embedding_matrix):\n",
    "    nlp_input = Input(shape=(maxlen,), name='nlp_input')\n",
    "    #meta_input = Input(shape=(3,), name='meta_input')\n",
    "    x = Embedding(embedding_matrix.shape[0],embedding_matrix.shape[1], weights=[embedding_matrix],trainable=False)(nlp_input)\n",
    "    x = Bidirectional(CuDNNGRU(128,return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(128,return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    x = concatenate([avg_pool, max_pool])\n",
    "    x = Dense(128, activation=\"relu\")(x)\n",
    "    outp = Dense(1, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=[nlp_input], outputs=outp)\n",
    "    model.summary()\n",
    "    opt = Adam(lr=2e-4)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy',f1_func])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_pred(model, epochs=10):\n",
    "    for e in range(epochs):\n",
    "        model.fit([train_X,train_meta_X], train_y, batch_size=128, epochs=20, validation_data=([val_X,val_meta_X],val_y),verbose=1,class_weight=cw_dict, callbacks=callbacks)\n",
    "        pred_val_y = model.predict([val_X], batch_size=128, verbose=0)\n",
    "\n",
    "        best_thresh = 0.5\n",
    "        best_score = 0.0\n",
    "        for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "            thresh = np.round(thresh, 2)\n",
    "            score = metrics.f1_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "            if score > best_score:\n",
    "                best_thresh = thresh\n",
    "                best_score = score\n",
    "\n",
    "        print(\"Val F1 Score: {:.4f}\".format(best_score))\n",
    "\n",
    "    #pred_test_y = model.predict([test_X], batch_size=1024, verbose=0)\n",
    "    return pred_val_y,best_score,best_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model_cnn_statistics(embedding_matrix)\n",
    "train_pred(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_val_y = model.predict([val_X], batch_size=256, verbose=0)\n",
    "\n",
    "best_thresh = 0.5\n",
    "best_score = 0.0\n",
    "for thresh in np.arange(0.1, 0.501, 0.01):\n",
    "    thresh = np.round(thresh, 2)\n",
    "    score = metrics.roc_auc_score(val_y, (pred_val_y > thresh).astype(int))\n",
    "    if score > best_score:\n",
    "        best_thresh = thresh\n",
    "        best_score = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
